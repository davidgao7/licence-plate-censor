# -*- coding: utf-8 -*-
"""vision-model-training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DuuQLWWxxKUy41VUX6DXucfGyz3qOP34

> In this notebook, I'm going to train a license plate detector model (OCR) using YOLOv8 and deployed it on device using SNPE

# Model
YOLO(You Only Look Once)v8 is a new object detection model, aming to further enhance performance and robustness, inspired by YOLO families.
- characteriestics
    * optimal speed and accuracy
    * improvements specifically trailored for **small** object detection, addressing challenges highlighted in YOLOv7
    * enhances video-based object detection
- paper credicts
    * R. Varghese and S. M., "YOLOv8: A Novel Object Detection Algorithm with Enhanced Performance and Robustness," 2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS), Chennai, India, 2024, pp. 1-6, doi: 10.1109/ADICS58448.2024.10533619. keywords: {YOLO;Performance evaluation;Technological innovation;Computer vision;Heuristic algorithms;Speech recognition;Benchmark testing;YOLOv8;Object Detection;Performance Enhancement;Robustness;Computational Efficiency;Computer Vision Systems},

# Prepare the data
"""

!pwd

!ls

!pip install ultralytics comet-ml

import ultralytics
ultralytics.checks()

"""# Prepare US licence plate data"""

import os
# https://github.com/AarohiSingla/YOLOv10-Custom-Object-Detection/tree/main/custom_dataset/dataset

# download data
!wget https://github.com/AarohiSingla/YOLOv10-Custom-Object-Detection/archive/refs/heads/main.zip

!unzip -o main.zip -d datasets
!rm main.zip
# data_folders = ["test", "train", "valid"]

# # remove the files, folder except the data_folders
# for root, dirs, files in os.walk("data/"):
#     for dir in dirs:
#         if dir not in data_folders:
#             os.rmdir(os.path.join(root, dir))
#             print(f"Removed directory: {os.path.join(root, dir)}")

"""> train/valid data and labels have same format with yolov8 [official tutorial](https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb#scrollTo=X58w8JLpMnjH), we might can just use cmd to train"""

# Commented out IPython magic to ensure Python compatibility.
#@title Select YOLOv8 ðŸš€ logger {run: 'auto'}
logger = 'Comet' #@param ['Comet', 'TensorBoard']

if logger == 'Comet':
#   %pip install -q comet_ml
  import comet_ml; comet_ml.login()
elif logger == 'TensorBoard':
#   %load_ext tensorboard
#   %tensorboard --logdir .

"""# Train customized license plate detector"""

!ls

# check image size
img_dir = "datasets/YOLOv10-Custom-Object-Detection-main/custom_dataset/dataset/train/images"
img = os.listdir(img_dir)[0]
img_path = os.path.join(img_dir, img)

import cv2
img = cv2.imread(img_path)
print(img.shape)

from ultralytics import YOLO

# Create a new YOLO model from scratch
model = YOLO("yolov8m.yaml")

# Load a pretrained YOLO model (recommended for training)
model = YOLO("yolov8m.pt")

results = model.train(data="/content/datasets/YOLOv10-Custom-Object-Detection-main/custom_data.yaml", epochs=20)

# Evaluate the model's performance on the validation set
results = model.val()

# Export the model to ONNX format
success = model.export(format="onnx")

model

success

results

# Commented out IPython magic to ensure Python compatibility.
# # use cmd to train model(but u don't have much control on it)
# # !yolo task=detect mode=train model=yolov8m.pt data=/content/datasets/YOLOv10-Custom-Object-Detection-main/custom_data.yaml epochs=20 imgsz=640
# %%bash
# export LC_ALL=C.UTF-8
# ls

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# export LC_ALL=C.UTF-8
# pwd

# %%bash
# # test get images
# export LC_ALL=C.UTF-8
# wget https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims3%2FGLOB%2Flegacy_thumbnail%2F1130x636%2Fformat%2Fjpg%2Fquality%2F85%2Fhttps%3A%2F%2Fs.aolcdn.com%2Fos%2Fab%2F_cms%2F2021%2F06%2F29163234%2FLicense-Plates-Tenn.jpg&f=1&nofb=1&ipt=8beb0286e511799d39bb1c2be03d498aed2350546bd758b283db271862fa5371&ipo=images

# use model to predict
import cv2
from ultralytics import YOLO

# test labeling several images
test_img_dir = "test_licence_plates_us"

# plot images in a 3x3 grid
import cv2
import matplotlib.pyplot as plt

import os

img_paths = []
img_names = []

os.system(f"mkdir {test_img_dir}")
os.system(f"""cd {test_img_dir} && \
            wget -O img1.jpg https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims3%2FGLOB%2Flegacy_thumbnail%2F1130x636%2Fformat%2Fjpg%2Fquality%2F85%2Fhttps%3A%2F%2Fs.aolcdn.com%2Fos%2Fab%2F_cms%2F2021%2F06%2F29163234%2FLicense-Plates-Tenn.jpg&f=1&nofb=1&ipt=8beb0286e511799d39bb1c2be03d498aed2350546bd758b283db271862fa5371&ipo=images""")
os.system(f"""cd {test_img_dir} && \
wget -O img2.jpg https://www.aclu.org/sites/default/files/field_image/va_plate_by_tony_alter_credit2.jpg""")
os.system(f"""cd {test_img_dir} && \
wget -O img3.jpg https://st.hotrod.com/uploads/sites/21/2015/07/personalized-license-plates-from-mustang-week-2015-28.jpg""")

for img in os.listdir(test_img_dir):
    if not img.endswith(".jpg"):
        continue
    img_names.append(img)
    img_path = os.path.join(test_img_dir, img)
    img_paths.append(img_path)

# # batched inference
# if not model:
#     device = qai_hub.Device(selected_device)

# # Compile for target device
# compile_job = qai_hub.submit_compile_job(
#     model=traced_model,                        # Traced PyTorch model
#     input_specs={"image": input_shape},        # Input specification
#     device=device,                             # Device
# )

# load model if not train
if 'model' not in globals():
    model = YOLO("best.pt")

def pixelation(img, x1, y1, x2, y2):
    """
    Pixelates a region of an image.
    Args:
        img: The image to pixelate.
        x1: The x-coordinate of the top-left corner of the region to pixelate.
        y1: The y-coordinate of the top-left corner of the region to pixelate.
        x2: The x-coordinate of the bottom-right corner of the region to pixelate.
        y2: The y-coordinate of the bottom-right corner of the region to pixelate.

    Returns:
        The pixelated image.
    """
    # remove the batch dimension
    img = img.copy() # Create a copy to avoid modifying the original image

    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # Convert coordinates to integers

    w = x2 - x1
    h = y2 - y1

    # Extract the region of interest (ROI)
    roi = img[y1:y1+h, x1:x1+w]

    # Pixelate the ROI by resizing to a smaller size and then back to the original size
    small = cv2.resize(roi, (16, 16), interpolation=cv2.INTER_LINEAR)
    pixelated = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)

    # Replace the original region with the pixelated one
    img[y1:y1+h, x1:x1+w] = pixelated

    return img


def plot_censor_plate(boxes, img):

    print(f"boxes: {boxes}")
    print(f"img: {img.shape}")

    for box in boxes:
        x1, y1, x2, y2 = box
        img = pixelation(img, x1, y1, x2, y2)

    return img


# save result image

results = model(img_paths)

img_censor_paths = []
img_box_paths = []

for result,img_name in zip(results,img_names):
    boxes = result.boxes  # boxes objct for bounding box outputs
    masks = result.masks  # masks objct for segmentation masks outputs
    keypoints = result.keypoints  # keypoints objct for pose outputs
    probs = result.probs  # class probabilities for classification outputs
    obb = result.obb  # oriented bounding boxes
    img_box = result.plot()  # plot a BGR numpy array of predictions
    # print(boxes, masks, keypoints, probs, obb)
    # print("======================================")

    # apply censoring
    img_name = img_name.split(".")[0]
    print(test_img_dir+"/"+img_name+".jpg")
    img = cv2.imread(test_img_dir+"/"+img_name+".jpg")
    print(img.shape) # (1,3,640,640)

    cv2.imwrite(test_img_dir+"/"+img_name+"_box.jpg", img_box)

    img_censor = plot_censor_plate(boxes.xyxy.tolist(), img)
    cv2.imwrite(test_img_dir+"/"+img_name+"_box_censor.jpg", img_censor)

    img_box_paths.append(test_img_dir+"/"+img_name+"_box.jpg")
    img_censor_paths.append(test_img_dir+"/"+img_name+"_box_censor.jpg")

    result.save(test_img_dir+"/"+img_name+"_box.jpg")

img_paths

img_box_paths

from matplotlib_inline import backend_inline
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

import cv2

fig, axes = plt.subplots(nrows=3, ncols=len(img_paths), figsize=(20,5))

for ax, img_path in zip(axes[0], img_paths):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    ax.imshow(img)

for ax, img_path in zip(axes[1], img_box_paths):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    ax.imshow(img)

for ax, img_path in zip(axes[2], img_censor_paths):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    ax.imshow(img)

plt.tight_layout()
plt.show()



"""# model on-device deployment"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# export LC_ALL=C.UTF-8
# 
# # install qualcomm lib
# pip install qai_hub

# # load model
# import torch
# from ultralytics import YOLO

# if not model:
#     model = YOLO("runs/detect/train/weights/best.pt")

# import qai_hub
# import getpass

# ai_hub_api_token = getpass.getpass("Enter your AI Hub API token: ")

# import os
# os.system(f"qai-hub configure --api_token {ai_hub_api_token}")

# for device in qai_hub.get_devices():
#     print(device.name)

# """
# the error occurs because certain operations in the YOLO model, such as rounding on tensors, are not compatible with TorchScript tracing (torch.jit.trace). This is a common issue when trying to trace YOLO models, including various versions like YOLOv5 and YOLOv7.
# """

# import qai_hub as hub

# model = YOLO("runs/detect/train/weights/best.pt")
# traced_model = torch.jit.trace(model, torch.zeros(1, 3, 640, 640))

# # compile the model for a specific Qualcomm device
# compile_job = hub.submit_compile_job(
#     model,
#     name="licencePlateRecon",
#     device=hub.Device("Google Pixel 6"),
#     input_specs={"image": (1, 3, 640, 640)},
# )

# %%bash
# export LC_ALL=C.UTF-8

# # install qualcomm lib
# pip install flask-ngrok

# import torch
# from torchvision import transforms
# from flask import Flask, request, jsonify
# # from flask_ngrok import run_with_ngrok
# import numpy as np
# import cv2

# # Load the PyTorch model
# model = torch.load("/content/runs/detect/train/weights/best.pt")

# app = Flask(__name__)
# run_with_ngrok(app)  # Start ngrok when the app is run


# @app.route("/predict", methods=["POST"])
# def predict():
#     # Check if the POST request has the file part
#     if "file" not in request.files:
#         return jsonify({"error": "No file part"}), 400
#     file = request.files["file"]
#     if file.filename == "":
#         return jsonify({"error": "No selected file"}), 400

#     # Read the image file
#     img_bytes = file.read()
#     nparr = np.frombuffer(img_bytes, np.uint8)
#     image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

#     # Preprocess the image for the model
#     # Assuming model expects a 3x224x224 input
#     # Convert to tensor and normalize
#     transform = transforms.Compose(
#         [
#             transforms.ToPILImage(),
#             transforms.Resize((224, 224)),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
#         ]
#     )
#     input_tensor = transform(image).unsqueeze(0)  # Add batch dimension

#     # Perform inference
#     with torch.no_grad():
#         output = model(input_tensor)

#     # Process the output (mock processing here)
#     # You might need to apply softmax or argmax depending on your model
#     prediction = torch.argmax(output, dim=1).item()

#     return jsonify({"output": prediction})


# if __name__ == "__main__":
#     app.run()

# %%bash
# export LC_ALL=C.UTF-8


# pip list | grep -E "torch|cv"

# load pretrained model
from ultralytics import YOLO
model = YOLO("best.pt")

!pip freeze > requirements.txt

